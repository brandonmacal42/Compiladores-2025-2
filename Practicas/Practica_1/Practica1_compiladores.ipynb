{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a67f1a39",
      "metadata": {
        "id": "a67f1a39"
      },
      "source": [
        "# Práctica 1: Expresiones regulares y lexers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8859dbe-b61a-4c13-8051-7bba85f9be7b",
      "metadata": {
        "id": "d8859dbe-b61a-4c13-8051-7bba85f9be7b"
      },
      "source": [
        "## 1. Realiza un sistema para identificar patrones dentro de cadenas textuales a partir de expresiones regulares"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e94f85-eac1-455c-85f2-55fe2eefe8a7",
      "metadata": {
        "id": "21e94f85-eac1-455c-85f2-55fe2eefe8a7"
      },
      "source": [
        "### a. Realiza una clase que simule a un autómata finito (puede ser no determinístico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3592a7e8-0428-461d-8265-7385fcaa6831",
      "metadata": {
        "id": "3592a7e8-0428-461d-8265-7385fcaa6831"
      },
      "outputs": [],
      "source": [
        "class Automata(object):\n",
        "    def __init__(self, states, initial, final, transitions):\n",
        "        self.states = states\n",
        "        self.initial = initial\n",
        "        self.final = final\n",
        "        self.transitions = transitions\n",
        "    \"\"\"Clase para objeto Autómata finito\"\"\"\n",
        "\n",
        "\n",
        "    def accepts(self, cadena):\n",
        "        q_actual = self.initial\n",
        "        for caracter in cadena:\n",
        "            if(q_actual, caracter) in self.transitions:\n",
        "                q_actual = self.transitions[(q_actual, caracter)]\n",
        "            else:\n",
        "                return False\n",
        "            return q_actual in self.final\n",
        "    \"\"\"Método que acepta o rechaza una cadena según si esta pertenece o no al lenguaje\"\"\"\n",
        "\n",
        "    def match_token(self, texto):\n",
        "        posicion_token = []\n",
        "        for i in range(len(texto)):\n",
        "            q_actual = self.initial\n",
        "            for j in range(i, len(texto)):\n",
        "                if (q_actual, texto[j]) in self.transitions:\n",
        "                    q_actual = self.transitions[(q_actual, texto[j])]\n",
        "                    if q_actual in self.final:\n",
        "                        posicion_token.append((i, j+1))\n",
        "                else:\n",
        "                    break\n",
        "        return posicion_token\n",
        "    \"\"\"Regresa la posición de un tóken cuando este es aceptado por el autómata\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17df8dfc-192d-41cb-bd5b-0fa8b1a4357c",
      "metadata": {
        "id": "17df8dfc-192d-41cb-bd5b-0fa8b1a4357c"
      },
      "source": [
        "### b. Define la  función de construcción del autómata a partir de los patrones de expresiones regulares incluyendo los símbolos `| (or)`, `* (kleene)`, `+ (1 o más veces)` y `? (0 o 1 vez)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4a6e03-709e-4342-bcd6-0936250e4ad5",
      "metadata": {
        "id": "ad4a6e03-709e-4342-bcd6-0936250e4ad5"
      },
      "outputs": [],
      "source": [
        "def compile(regex: str) -> Automata:\n",
        "    \"\"\"Construye un autómata finito no determinista a partir de una expresión regular usando Thompson.\"\"\"\n",
        "    from collections import deque\n",
        "\n",
        "    state_counter = 0\n",
        "    def new_state():\n",
        "        nonlocal state_counter\n",
        "        state = state_counter\n",
        "        state_counter += 1\n",
        "        return state\n",
        "\n",
        "    stack = []\n",
        "    operators = []\n",
        "    transitions = {}\n",
        "\n",
        "    for char in regex:\n",
        "        if char.isalnum():  # Caracteres normales\n",
        "            s1, s2 = new_state(), new_state()\n",
        "            transitions[(s1, char)] = {s2}\n",
        "            stack.append((s1, {s2}))\n",
        "        elif char == '|':  # Alternancia\n",
        "            operators.append(char)\n",
        "        elif char == '*':  # Cierre de Kleene\n",
        "            s1, s2 = new_state(), new_state()\n",
        "            old_s1, old_finals = stack.pop()\n",
        "            transitions.setdefault((s1, ''), set()).update({old_s1, s2})\n",
        "            for f in old_finals:\n",
        "                transitions.setdefault((f, ''), set()).update({old_s1, s2})\n",
        "            stack.append((s1, {s2}))\n",
        "        elif char == '?':  # 0 o 1 ocurrencia\n",
        "            old_s1, old_finals = stack.pop()\n",
        "            s1, s2 = new_state(), new_state()\n",
        "            transitions.setdefault((s1, ''), set()).update({old_s1, s2})\n",
        "            for f in old_finals:\n",
        "                transitions.setdefault((f, ''), set()).add(s2)\n",
        "            stack.append((s1, {s2}))\n",
        "        elif char == '+':  # 1 o más veces\n",
        "            old_s1, old_finals = stack.pop()\n",
        "            s1, s2 = new_state(), new_state()\n",
        "            transitions.setdefault((s1, ''), set()).add(old_s1)\n",
        "            for f in old_finals:\n",
        "                transitions.setdefault((f, ''), set()).update({old_s1, s2})\n",
        "            stack.append((s1, {s2}))\n",
        "\n",
        "    # Procesar alternancia (|)\n",
        "    while operators:\n",
        "        op = operators.pop()\n",
        "        if op == '|':\n",
        "            s1, s2 = new_state(), new_state()\n",
        "            right_s1, right_finals = stack.pop()\n",
        "            left_s1, left_finals = stack.pop()\n",
        "            transitions.setdefault((s1, ''), set()).update({left_s1, right_s1})\n",
        "            for f in left_finals | right_finals:\n",
        "                transitions.setdefault((f, ''), set()).add(s2)\n",
        "            stack.append((s1, {s2}))\n",
        "\n",
        "    initial, final_states = stack.pop()\n",
        "    return Automata(set(range(state_counter)), initial, final_states, transitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79473c2-dce6-4cc6-bb71-48560bf11fa1",
      "metadata": {
        "id": "e79473c2-dce6-4cc6-bb71-48560bf11fa1"
      },
      "source": [
        "### c. Prueba el autómata con el siguiente patrón y sobre las siguientes cadenas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad00bdc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ad00bdc4",
        "outputId": "1acac03e-2380-4e88-8dcf-89da8ea1432b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Automata'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8f8f53783a20>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mAutomata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Construye el autómata de la regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'niña|os?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Automata'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from automata import compile, tokenize\n",
        "\n",
        "# Construye el autómata de la regex\n",
        "pattern = compile('niña|os?')\n",
        "\n",
        "#Cadenas que acepta\n",
        "print(pattern.accepts('niño'))\n",
        "print(pattern.accepts('niña'))\n",
        "print(pattern.accepts('niñas'))\n",
        "print(pattern.accepts('niños'))\n",
        "print(pattern.accepts('niñs'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab48f436",
      "metadata": {
        "id": "ab48f436"
      },
      "source": [
        "### d. Utiliza el autómata para localizar los patrones que coinciden en el siguiente texto (la función tokenize utiliza los espacios en blanco para obtener tókens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd0fc97b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cd0fc97b",
        "outputId": "3c47b05f-5f32-4f38-c5e6-bc52177a1a3e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenize' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9bc9f9612df2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenización del texto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Las niñas extranjeras jugaban con el niño y la niña en el patio.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Encuentra las correspondecias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
          ]
        }
      ],
      "source": [
        "#llamamos a las funciones\n",
        "from automata import compile, tokenize\n",
        "\n",
        "#definimos los lexers para tomar las palabras 1 niñas, 6 niño, 9 niña\n",
        "pattern = compile('niña|os?')\n",
        "# Tokenización del texto\n",
        "text = tokenize('Las niñas extranjeras jugaban con el niño y la niña en el patio.')\n",
        "\n",
        "\n",
        "\n",
        "# Encuentra las correspondecias\n",
        "matches = pattern.match_token(text)\n",
        "\n",
        "# Imprime índices y tokenes\n",
        "# que cumplen el patrón dentro del texto\n",
        "for i in matches:\n",
        "    print(i, text[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e2d22f",
      "metadata": {
        "id": "f4e2d22f"
      },
      "source": [
        "## 2. Utilizando lex, construye un lexer que pueda identificar los siguientes tókens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cb11fb-a8d4-4aa0-a8f3-ae6c6755b853",
      "metadata": {
        "id": "f2cb11fb-a8d4-4aa0-a8f3-ae6c6755b853"
      },
      "source": [
        "- Identificadores\n",
        "- Operadores\n",
        "    - binarios\n",
        "    - unarios\n",
        "    - relacionales\n",
        "- Keywords\n",
        "- Número\n",
        "  - enteros\n",
        "  - flotantes\n",
        "- Literales\n",
        "- Signos de puntuación\n",
        "- Booleanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24341905",
      "metadata": {
        "id": "24341905"
      },
      "outputs": [],
      "source": [
        "%option noyywrap\n",
        "%{\n",
        "#include <stdio.h>\n",
        "%}\n",
        "\n",
        "%%\n",
        "[a-zA-Z_][a-zA-Z0-9_]*  { printf(\"IDENTIFICADOR: %s\\n\", yytext); }\n",
        "\n",
        "\\+|\\-|\\*|\\/|%        { printf(\"OPERADOR_BINARIO: %s\\n\", yytext); }\n",
        "\\+\\+|\\-\\-            { printf(\"OPERADOR_UNARIO: %s\\n\", yytext); }\n",
        "==|!=|<=|>=|<|>       { printf(\"OPERADOR_RELACIONAL: %s\\n\", yytext); }\n",
        "\n",
        "[ \\t\\n]              ; /* Ignorar espacios y saltos de línea */\n",
        ".                   { printf(\"OTRO: %s\\n\", yytext); }\n",
        "%%\n",
        "\n",
        "int main() {\n",
        "    printf(\"Ingresa una expresión:\\n\");\n",
        "    yylex();\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
